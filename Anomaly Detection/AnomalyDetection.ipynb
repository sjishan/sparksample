{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#iPython Notebook and Apache Spark path appending\n",
    "os.environ['SPARK_HOME']=\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6\"\n",
    "sys.path.append(\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6/bin\")\n",
    "sys.path.append(\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6/python\")\n",
    "sys.path.append(\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6/python/pyspark\")\n",
    "sys.path.append(\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6/python/lib\")\n",
    "sys.path.append(\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip\")\n",
    "sys.path.append(\"S:/spark/spark-1.5.2-bin-hadoop2.6/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\")\n",
    "sys.path.append(\"C:/Program Files/Java/jre1.8.0_60/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "import operator\n",
    "SparkContext.setSystemProperty(\"hadoop.home.dir\", \"S:\\\\spark\\\\winutil\\\\\") #For Windows OS only\n",
    "conf = SparkConf().setAppName(\"AnomalyDetection\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlCt = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Goal is to detect an anomaly within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just a toy dataset\n",
    "data = [(0, [\"http\", \"udt\", 0.4]), \n",
    "        (1, [\"http\", \"udf\", 0.5]), \n",
    "        (2, [\"http\", \"tcp\", 0.5]), \n",
    "        (3, [\"ftp\", \"icmp\", 0.1]), \n",
    "        (4, [\"http\", \"tcp\", 0.4])]\n",
    "schema = [\"id\", \"rawFeatures\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlCt.createDataFrame(data, schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|     rawFeatures|\n",
      "+---+----------------+\n",
      "|  0|[http, udt, 0.4]|\n",
      "|  1|[http, udf, 0.5]|\n",
      "|  2|[http, tcp, 0.5]|\n",
      "|  3|[ftp, icmp, 0.1]|\n",
      "|  4|[http, tcp, 0.4]|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##First: Change the categorical features to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices =[0,1]\n",
    "dlist = []\n",
    "for idx in indices:\n",
    "    catUDF = udf(lambda x: x[idx], StringType())\n",
    "    #for each column only collect the unique entries ex: Column 0 has http, ftp\n",
    "    rows = df.select(catUDF(df.rawFeatures)).distinct().collect()\n",
    "    d = {}\n",
    "    for i, r in enumerate(rows):\n",
    "        d[r[0]] = [0.0]*len(rows)  #list of size 2, creates [0.0, 0,0] for Column 0\n",
    "        d[r[0]][i] = 1.0 #let's say http is the first value encountered then i is 0 at that point so it will be [1.0,0.0]\n",
    "    dlist.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'ftp': [1.0, 0.0], u'http': [0.0, 1.0]},\n",
       " {u'icmp': [0.0, 0.0, 1.0, 0.0],\n",
       "  u'tcp': [0.0, 1.0, 0.0, 0.0],\n",
       "  u'udf': [0.0, 0.0, 0.0, 1.0],\n",
       "  u'udt': [1.0, 0.0, 0.0, 0.0]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Second: Apply one hot encoding to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlistBC = sc.broadcast(dlist)\n",
    "def transform(features):\n",
    "    newFeatures = []\n",
    "    for i,idx in enumerate(indices):\n",
    "        print i, idx\n",
    "        newFeatures.extend(dlistBC.value[i][features[idx]])  #ex: dlistBC.value[0]['http'] add the one hot encoding in the list\n",
    "    newFeatures.extend([float(x) for x in features[2:]]) #add the last numerical column\n",
    "    return newFeatures\n",
    "        \n",
    "transformUDF = udf(transform, ArrayType(FloatType()))\n",
    "df = df.withColumn(\"features\", transformUDF(df.rawFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.0, 1.0, 1.0, 0...|\n",
      "|[0.0, 1.0, 0.0, 0...|\n",
      "|[0.0, 1.0, 0.0, 1...|\n",
      "|[1.0, 0.0, 0.0, 0...|\n",
      "|[0.0, 1.0, 0.0, 1...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Third: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = df.select(\"features\").map(lambda row: row[0]).cache()\n",
    "model = KMeans.train(features, k=2, maxIterations=40, runs=10, initializationMode=\"random\", seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelBC = sc.broadcast(model)\n",
    "clusterUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "df = df.withColumn(\"cluster\", clusterUDF(df.features)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------------+-------+\n",
      "| id|     rawFeatures|            features|cluster|\n",
      "+---+----------------+--------------------+-------+\n",
      "|  0|[http, udt, 0.4]|[0.0, 1.0, 1.0, 0...|      0|\n",
      "|  1|[http, udf, 0.5]|[0.0, 1.0, 0.0, 0...|      0|\n",
      "|  2|[http, tcp, 0.5]|[0.0, 1.0, 0.0, 1...|      0|\n",
      "|  3|[ftp, icmp, 0.1]|[1.0, 0.0, 0.0, 0...|      1|\n",
      "|  4|[http, tcp, 0.4]|[0.0, 1.0, 0.0, 1...|      0|\n",
      "+---+----------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For bigger dataset with more clusters scoring system is required, like for example:\n",
    "### score (X) = (sizeOfMaxCluster - sizeOfClusterX) / (sizeOfMaxCluster - sizeOfMinCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
